{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6ae2c16-93c3-44ab-b0f8-91045f93c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "from typing import Literal\n",
    "\n",
    "import pyscenarios\n",
    "from pyscenarios._sobol._kernel_numba import sobol_kernel as sobol_numba\n",
    "from pyscenarios._sobol._kernel_numpy import sobol_kernel as sobol_numpy, _calc_c as _c_numpy\n",
    "from pyscenarios._sobol._vmatrix import V\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.lax\n",
    "import jax.numpy as jnp\n",
    "import numba\n",
    "import scipy\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b555a9-6a45-40a8-8fdd-51dd6af62590",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES = 2**17 - 1\n",
    "DIMENSIONS = 500\n",
    "CHUNKS = (2**13, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f38f0a-47d8-447f-ac84-00d07c4ddbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jax_backend() -> Literal[\"cpu\", \"gpu\", \"tpu\"]:\n",
    "    \"\"\"Determine the backend on which new arrays are created with device=None.\n",
    "\n",
    "    Note that device=not None does not work as of JAX 0.6.0:\n",
    "    https://github.com/jax-ml/jax/issues/26000\n",
    "\n",
    "    .. warning::\n",
    "\n",
    "       This function relies on the fact that `@jax.jit` re-traces the Python\n",
    "       when the default device changes, which is an implementation detail and\n",
    "       may break without warning in a future version of JAX.\n",
    "    \"\"\"\n",
    "    device = jax.config.jax_default_device\n",
    "    return jax.default_backend() if device is None else device.platform\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f():\n",
    "    backend = get_jax_backend()\n",
    "    return jnp.asarray(1 if backend == \"cpu\" else 2)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def g(x):\n",
    "    backend = get_jax_backend()\n",
    "    return x + (1 if backend == \"cpu\" else 2)\n",
    "\n",
    "f.clear_cache()\n",
    "g.clear_cache()\n",
    "\n",
    "cpu = jax.devices(\"cpu\")[0]\n",
    "gpu = jax.devices(\"gpu\")[0]\n",
    "\n",
    "x_cpu = jnp.asarray(0, device=cpu)\n",
    "x_gpu = jnp.asarray(0, device=gpu)\n",
    "\n",
    "jax.config.update(\"jax_default_device\", cpu)\n",
    "assert f() == 1\n",
    "assert f().device.platform == \"cpu\"\n",
    "assert g(x_cpu) == 1\n",
    "assert g(x_cpu).device.platform == \"cpu\"\n",
    "assert g(x_gpu) == 1\n",
    "assert g(x_gpu).device.platform == \"gpu\"\n",
    "\n",
    "jax.config.update(\"jax_default_device\", gpu)\n",
    "assert f() == 2\n",
    "assert f().device.platform == \"gpu\"\n",
    "assert g(x_cpu) == 2\n",
    "assert g(x_cpu).device.platform == \"cpu\"\n",
    "assert g(x_gpu) == 2\n",
    "assert g(x_gpu).device.platform == \"gpu\"\n",
    "\n",
    "jax.config.update(\"jax_default_device\", None)\n",
    "assert f() == 2\n",
    "assert f().device.platform == \"gpu\"\n",
    "assert g(x_cpu) == 2\n",
    "assert g(x_cpu).device.platform == \"cpu\"\n",
    "assert g(x_gpu) == 2\n",
    "assert g(x_gpu).device.platform == \"gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90fb277d-70af-431f-9a50-a4528caac70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _c_aapi(samples: int, *, device=None, xp):\n",
    "    \"\"\"c[i] = index from the right of the first zero bit of sample index i\"\"\"\n",
    "    # c[i] = index from the right of the first zero bit of sample index i\n",
    "    samples_rng = xp.arange(samples, device=device)\n",
    "    c_max = int(math.log(samples, 2))\n",
    "    out = xp.full(samples, c_max, device=device)\n",
    "    for c in range(c_max + 1, -1, -1):\n",
    "        mask = samples_rng & (1 << c) == 0\n",
    "        out = xp.where(mask, c, out)\n",
    "    return out\n",
    "\n",
    "\n",
    "_c_aapi_jax = jax.jit(partial(_c_aapi, xp=jnp), static_argnames=(\"samples\", \"device\"))\n",
    "\n",
    "expect = _c_numpy(SAMPLES)\n",
    "np.testing.assert_array_equal(_c_aapi(SAMPLES, xp=np), expect, strict=True)\n",
    "for device in (cpu, gpu):\n",
    "    jax.config.update(\"jax_default_device\", device)\n",
    "    assert _c_aapi_jax(SAMPLES).device == device\n",
    "    np.testing.assert_array_equal(_c_aapi_jax(SAMPLES), expect, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ec1711c-7258-47b2-9b25-ea16491762ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy\n",
      "2.91 ms ± 11.6 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "JAX cpu\n",
      "50.4 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "75.3 μs ± 4.63 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "JAX gpu\n",
      "92.8 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "41.6 μs ± 306 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# BENCHMARK\n",
    "\n",
    "print(\"NumPy\")\n",
    "%timeit _c_aapi(SAMPLES, xp=np)\n",
    "\n",
    "_c_aapi_jax.clear_cache()\n",
    "for device in (cpu, gpu):\n",
    "    print(f\"\\nJAX {device.platform}\")\n",
    "    jax.config.update(\"jax_default_device\", device)\n",
    "    %timeit -n 1 -r 1 _c_aapi_jax(SAMPLES).block_until_ready()\n",
    "    %timeit _c_aapi_jax(SAMPLES).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e932fd6e-b5fb-4d29-a67a-3a17b86f7ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=(\"samples\", \"dimensions\", \"d0\"))\n",
    "def sobol_jax(samples: int, dimensions: int, d0: int = 0):\n",
    "    c = _c_aapi(samples, xp=jnp)\n",
    "    VT = jnp.asarray(V)[d0:d0 + dimensions, :].T\n",
    "    states = jnp.take(VT, c, axis=0)\n",
    "\n",
    "    # FIXME fairly slow on CPU and very slow on GPU\n",
    "    # https://github.com/jax-ml/jax/issues/28097\n",
    "    # states = jnp.bitwise_xor.accumulate(states, axis=0)\n",
    "    if get_jax_backend() == \"cpu\":\n",
    "        # 1.2x faster on CPU and 1.3x faster on GPU\n",
    "        states = jax.lax.fori_loop(\n",
    "            1,\n",
    "            samples,\n",
    "            lambda i, x: x.at[i, :].set(x[i - 1, :] ^ x[i, :]),\n",
    "            states,\n",
    "        )\n",
    "    else:  # gpu, tpu\n",
    "        # 220x faster on GPU, but 2.5x slower on CPU\n",
    "        states = jax.lax.associative_scan(jnp.bitwise_xor, states)\n",
    "\n",
    "    return jnp.astype(states, jnp.float64) / 2**32\n",
    "\n",
    "\n",
    "expect = sobol_numba(1023, 4, 0, 0)\n",
    "\n",
    "for device in (cpu, gpu):\n",
    "    jax.config.update(\"jax_default_device\", device)\n",
    "    actual = sobol_jax(1023, 4)\n",
    "    assert actual.device == device\n",
    "    np.testing.assert_allclose(actual, expect, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b18031-8548-4128-b4a5-4856581421c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numba\n",
      "95.7 ms ± 832 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "Dask+Numba\n",
      "5.65 s ± 90.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "NumPy\n",
      "645 ms ± 26.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "JAX cpu\n",
      "256 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "194 ms ± 2.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "JAX gpu\n",
      "5.83 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "5.52 ms ± 122 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# BENCHMARK\n",
    "\n",
    "print(\"Numba\")\n",
    "%timeit pyscenarios.sobol((SAMPLES, DIMENSIONS))\n",
    "\n",
    "print(\"\\nDask+Numba\")\n",
    "%timeit pyscenarios.sobol((SAMPLES, DIMENSIONS), chunks=CHUNKS).compute()\n",
    "\n",
    "print(\"\\nNumPy\")\n",
    "%timeit sobol_numpy(SAMPLES, DIMENSIONS, 0, 0)\n",
    "\n",
    "sobol_jax.clear_cache()\n",
    "for device in (cpu, gpu):\n",
    "    print(f\"\\nJAX {device.platform}\")\n",
    "    jax.config.update(\"jax_default_device\", device)\n",
    "    %timeit -n 1 -r 1 sobol_jax(SAMPLES, DIMENSIONS).block_until_ready()\n",
    "    %timeit sobol_jax(SAMPLES, DIMENSIONS).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9352ced6-3903-41e7-82b9-e7018d1fe6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.09502161, -0.107505  ],\n",
       "       [-0.09502161,  1.        , -0.09512254],\n",
       "       [-0.107505  , -0.09512254,  1.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_positive_definite_cov_matrix(n: int) -> np.ndarray:\n",
    "    \"\"\"Generate a positive definite covariance matrix of the specified width.\"\"\"\n",
    "    # Generate a random matrix\n",
    "    A = np.random.uniform(-1, 1, size=(n, n))\n",
    "    # Symmetrize the matrix\n",
    "    A = (A + A.T) / 2\n",
    "    # Add n * I to ensure positive definiteness\n",
    "    A += n * np.eye(n)\n",
    "    # Normalize to set the diagonal to 1\n",
    "    D = np.diag(1 / np.sqrt(np.diag(A)))\n",
    "    cov = D @ A @ D\n",
    "    return cov\n",
    "\n",
    "cov = generate_positive_definite_cov_matrix(3)\n",
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bb61234-f231-450a-961c-85f9834f8344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import scipy.special\n",
    "\n",
    "if sys.version_info[:2] >= (3, 13):\n",
    "    CPU_COUNT = os.process_cpu_count()\n",
    "else:\n",
    "    CPU_COUNT = os.cpu_count()\n",
    "\n",
    "\n",
    "def apply_parallel_numpy_ufunc(ufunc, *args):\n",
    "    shape = jnp.broadcast_shapes(*(jnp.shape(x) for x in args))\n",
    "    dtype = jnp.result_type(*args)\n",
    "    args = tuple(jnp.astype(x, dtype, copy=False) for x in args)\n",
    "    out_type = jax.ShapeDtypeStruct(shape, dtype)\n",
    "\n",
    "    def callback(*args):\n",
    "        args = jax.device_get(args)\n",
    "        args = tuple(np.asarray(x) for x in args)\n",
    "        args = np.broadcast_arrays(*args)\n",
    "        args = tuple(np.reshape(x, -1) for x in args)\n",
    "        out = np.empty_like(args[0])\n",
    "\n",
    "        n = max(1, min(CPU_COUNT, out.size // 10_000))\n",
    "        with ThreadPoolExecutor(n) as ex:\n",
    "            for i in range(n):\n",
    "                ex.submit(ufunc, *(arg[i::n] for arg in args), out=out[i::n])\n",
    "\n",
    "        return np.reshape(out, shape)\n",
    "\n",
    "    return jax.pure_callback(callback, out_type, *args)\n",
    "\n",
    "\n",
    "# jax.scipy.special.betainc exists, but it's very slow\n",
    "# https://github.com/jax-ml/jax/issues/28547\n",
    "betainc = partial(apply_parallel_numpy_ufunc, scipy.special.betainc)\n",
    "\n",
    "# jax.scipy.special.gammaincinv does not exist\n",
    "# https://github.com/jax-ml/jax/issues/5350\n",
    "gammaincinv = partial(apply_parallel_numpy_ufunc, scipy.special.gammaincinv)\n",
    "\n",
    "\n",
    "# jax.scipy.special.stdtr does not exist\n",
    "def stdtr(df, t):\n",
    "    x = df / (t ** 2 + df)\n",
    "    tail = betainc(df / 2, 0.5, x) / 2\n",
    "    return jnp.where(t < 0, tail, 1 - tail)\n",
    "\n",
    "\n",
    "for func, scipy_func, caller in (\n",
    "    (betainc, scipy.special.betainc, lambda f, x: f(4.5, 0.5, x)),\n",
    "    (gammaincinv, scipy.special.gammaincinv, lambda f, x: f(4.5, x)),\n",
    "    (stdtr, scipy.special.stdtr, lambda f, x: f(9., x)),\n",
    "):\n",
    "    x = np.linspace(0, 1, 101)\n",
    "    expect = caller(scipy_func, x)\n",
    "\n",
    "    for device in (cpu, gpu):\n",
    "        jax.config.update(\"jax_default_device\", device)\n",
    "        actual = caller(jax.jit(func), x)\n",
    "        np.testing.assert_allclose(actual, expect, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14afb3fb-bf82-41e3-ad2d-b061c21ad379",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=(\"samples\", \"seed\"))\n",
    "def copula_jax(cov, df, samples, seed):\n",
    "    dimensions = cov.shape[0]\n",
    "    L = jnp.linalg.cholesky(cov)\n",
    "\n",
    "    if df is None:\n",
    "        y = sobol_jax(samples, dimensions, d0=seed)\n",
    "    else:\n",
    "        yr = sobol_jax(samples, dimensions + 1, d0=seed)\n",
    "        y = yr[:, :-1]\n",
    "        r = yr[:, -1:]\n",
    "\n",
    "    y = jax.scipy.stats.norm.ppf(y)\n",
    "    p = (L @ y.T).T\n",
    "    if df is None:  # Gaussian Copula\n",
    "        return p\n",
    "\n",
    "    s = 2 * gammaincinv(df/2, r)  # Same as stats.chi2.ppf(r, df)\n",
    "    z = jnp.sqrt(df / s) * p\n",
    "    # Convert t distribution to normal (0, 1)\n",
    "    u = stdtr(df, z)  # Same as stats.t.cdf(z, df)\n",
    "    return jax.scipy.stats.norm.ppf(u)\n",
    "\n",
    "\n",
    "expect_g = pyscenarios.gaussian_copula(cov, samples=10, rng=\"Sobol\")\n",
    "expect_t = pyscenarios.t_copula(cov, df=9, samples=10, rng=\"Sobol\")\n",
    "\n",
    "for device in (cpu, gpu):\n",
    "    jax.config.update(\"jax_default_device\", device)\n",
    "    actual = copula_jax(cov, df=None, samples=10, seed=0)\n",
    "    assert actual.device == device\n",
    "    np.testing.assert_allclose(actual, expect_g, strict=True)\n",
    "\n",
    "    actual = copula_jax(cov, df=9, samples=10, seed=0)\n",
    "    assert actual.device == device\n",
    "    np.testing.assert_allclose(actual, expect_t, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a25bdbf1-fb9e-4700-8e6b-bd5dcd333385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numba Gaussian Copula\n",
      "5.57 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "\n",
      "Dask+Numba Gaussian Copula\n",
      "8.39 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "\n",
      "JAX Gaussian Copula cpu\n",
      "533 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "487 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "\n",
      "JAX Gaussian Copula gpu\n",
      "4.58 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "217 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "\n",
      "Numba T Copula\n",
      "29.1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "\n",
      "Dask+Numba T Copula\n",
      "5.28 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "\n",
      "JAX T Copula cpu\n",
      "2.05 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "1.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "\n",
      "JAX T Copula gpu\n",
      "4.06 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "1.66 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BENCHMARK\n",
    "\n",
    "cov = generate_positive_definite_cov_matrix(DIMENSIONS)\n",
    "copula_jax.clear_cache()\n",
    "\n",
    "for label, df, np_func in (\n",
    "    (\"Gaussian Copula\", None, pyscenarios.gaussian_copula),\n",
    "    (\"T Copula\", 9, partial(pyscenarios.t_copula, df=9)),\n",
    "):\n",
    "    print(f\"Numba {label}\")\n",
    "    %timeit -n 1 -r 1 np_func(cov, samples=SAMPLES, rng=\"Sobol\")\n",
    "\n",
    "    print(f\"\\nDask+Numba {label}\")\n",
    "    %timeit -n 1 -r 1 np_func(cov, samples=SAMPLES, rng=\"Sobol\", chunks=CHUNKS).compute()\n",
    "\n",
    "    for device in (cpu, gpu):\n",
    "        print(f\"\\nJAX {label} {device.platform}\")\n",
    "        jax.config.update(\"jax_default_device\", device)\n",
    "        %timeit -n 1 -r 1 copula_jax(cov, df, SAMPLES, DIMENSIONS).block_until_ready()\n",
    "        %timeit -n 1 -r 1 copula_jax(cov, df, SAMPLES, DIMENSIONS).block_until_ready()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a801174-dfbd-4354-8cae-f12a45410da7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
